
## 问题
```
在真实的工业界场景中，通常面临标注成本昂贵、泛化迁移能力不足、可解释性不强、计算资源受限、嵌套实体的长尾等问题，

词汇增强、冷启动、泛化性、低资源、噪声、不平衡、领域迁移、可解释、低耗时、否定性

样本少、不均衡、关键词信息融入文本分类（知识库/外部信息融合）


什么工作重要？为什么重要？
达到上限了么？
哪些方面有空间？
能不能迁移，泛化？
否定式能不能做？
非典型场景如何做（缺资源少数据few-shot，冷启动，specific 场景）



问题分析
	badcase，对应解决
	影响哪些
	上限下限
	相应占比

不同医院、不同疾病、不同科室的文本描述形式不一致，而标注成本又很昂贵，一个通用的NER系统往往不具备“想象中”的泛化迁移能力。当前的NER技术在医疗领域并不适合做成泛化的工具。
由于医疗领域的严肃性，我们既要知其然、更要知其所以然：NER系统往往不能采用“一竿子插到底”的黑箱算法，处理过程应该随着处理对象的层次和深度而逐步叠加模块，下级模块使用上级结果，方便进行迭代优化、并具备可解释性，这样做可解耦医学事件、也便于进行医学实体消歧。
仅仅使用统计模型的NER系统往往不是万能的，医疗领域相关的实体词典和特征挖掘对NER性能也起着关键作用。此外，NER结果往往不能直接使用，还需进行医学术语标准化。
由于医院数据不可出院，需要在院内部署NER系统。而通常医院内部的GPU计算资源又不是很充足（成本问题），我们需要让机器学习模型又轻又快（BERT上不动哇），同时要更充分的利用显存。



todo:
	知识多分发 + 模型分离
	解码限制
	normalization
	单位换算
	图像题目
		三角形和四边形，表格什么的
	特殊类型题目
		公式包裹
		大小题，第三步用到第一步，模型递归人循环
		
```

## 数据
```

核心是在粒度和数据量之间做平衡。数据量不足，就要粒度大一些，让模型学的简单。
数据量充足，就粒度小一些，让模型学复杂些。

数据不足
	模型
	    模型简化
	        整体当做一个预测而不是每个location(tex,nsp当做整体表示）
		独自建模
		    latex 专用embed
		数字都统一到单个拆分
		生成->ner->分类->
	数据
	    精简类别
	    规则化
	    强化特征，重点特征 重复
        数据归一化
            DATE、CURRENCY、EMAIL
	

数据不平衡
	让每个batch都能看到所有类别的样本
	增加权重
	清洗数据，去除噪声，降低负样本数据
	核心点还是难以区分的样本，学习到重点特征

UNK问题
	BPE https://zhuanlan.zhihu.com/p/86965595	
		1 准备足够大的训练语料
		2 确定期望的subword词表大小
		3 将单词拆分为字符序列并在末尾添加后缀“ </ w>”，统计单词频率。 本阶段的subword的粒度是字符。 例如，“ low”的频率为5，那么我们将其改写为“ l o w </ w>”：5
		4 统计每一个连续字节对的出现频率，选择最高频者合并成新的subword
		5 重复第4步直到达到第2步设定的subword词表大小或下一个最高频的字节对出现频率为1
		6 停止符"</w>"的意义在于表示subword是词后缀。举例来说："st"字词不加"</w>"可以出现在词首如"st ar"，加了"</w>"表明改字词位于词尾，如"wide st</w>"，二者意义截然不同。

		每次合并后词表可能出现3种变化：

		+1，表明加入合并后的新字词，同时原来的2个子词还保留（2个字词不是完全同时连续出现）
		+0，表明加入合并后的新字词，同时原来的2个子词中一个保留，一个被消解（一个字词完全随着另一个字词的出现而紧跟着出现）
		-1，表明加入合并后的新字词，同时原来的2个子词都被消解（2个字词同时连续出现）
		实际上，随着合并的次数增加，词表大小通常先增加后减小。

```
## 模型

### ICSF
BERT for Joint Intent Classification and Slot Filling
		

### pointer network
对于抽取，复制较多的情况，使用此机制合理  
https://zhuanlan.zhihu.com/p/48959800 

## 数据增强库
```
https://www.youtube.com/watch?v=9O9scQb4sNo 
https://github.com/varinf/TransformersDataAugmentation
NER数据增强
DAGA:https://www.aclweb.org/anthology/2020.emnlp-main.488.pdf
通用数据增强：https://github.com/varinf/TransformersDataAugmentation
分类数据增强：CBERT
https://github.com/1024er/cbert_aug
```

## 多分类
```

https://towardsdatascience.com/pytorch-tabular-multiclass-classification-9f8211a123ab
WeightedRandomSampler
https://blog.csdn.net/tyfwin/article/details/108435756


https://blog.csdn.net/xiaohuihui1994/article/details/93049975
https://www.cnblogs.com/wynlfd/p/14101373.html
pytorch 做多分类和多类别
BCEWithLogitsLoss用于单标签二分类或者多标签二分类
CrossEntropyLoss用于多类别分类，输出和目标的维度是(batch,C)，batch是样本数量，C是类别数量，每一个C之间是互斥的，相互关联的，对于每一个batch的C个值，一起求每个C的softmax
```