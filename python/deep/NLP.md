## subword怎么拆
https://zhuanlan.zhihu.com/p/86965595
从多字节拆分到单字节
添加@@ 是在拆分字符的时候，标记这个是子词仅仅分割作用


## Attention
Self-Attention的核心是用文本中的其它词来增强目标词的语义表示，从而更好的利用上下文的信息。
在相同量级的情况下，qi与ki点积的值会是最大的（可以从“两数和相同的情况下，两数相等对应的积最大”类比过来）。

### 【为什么除以dk?】
假设两个 dk 维向量每个分量都是一个相互独立的服从标准正态分布的随机变量，那么他们的点乘结果会变得很大，并且服从均值为0，方差就是 dk，【很大的点乘会让softmax函数处在梯度很小的区域】，对每一个分量除以 sqrt(d_k) 可以让点乘的方差变成 1。当 dk 比较大时点乘结果太大导致有效梯度太大（或被 clipped）。假设两个 d_k 维向量每个分量都是一个相互独立的服从标准正态分布的随机变量，那么他们的点乘的方差就是 d_k，每一个分量除以 sqrt(d_k) 可以让点乘的方差变成 1。