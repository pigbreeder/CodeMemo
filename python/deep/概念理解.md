
## 维度
pytorch Linear => y=xA^t+b
矩阵乘法bmm和matmul
https://pytorch.org/docs/stable/generated/torch.bmm.html
https://pytorch.org/docs/stable/generated/torch.matmul.html
>>> # batched matrix x batched matrix
>>> tensor1 = torch.randn(10, 3, 4)
>>> tensor2 = torch.randn(10, 4, 5)
>>> torch.matmul(tensor1, tensor2).size()
torch.Size([10, 3, 5])

a.shape=batch,len,dim，这个从最外围的维度抽就是一个句子，
a.permute(1,0,2)，这个从最外围抽就是batch所有第一个字符，
a.permute(2,1,0)，这个从最外围抽就是第一个dim的值所有字符所有句子，
In [3]: a=t.arange(12).reshape(2,3,2)

In [4]: a
Out[4]:
tensor([[[ 0,  1],
         [ 2,  3],
         [ 4,  5]],

        [[ 6,  7],
         [ 8,  9],
         [10, 11]]])

In [5]: a.permute(1,0,2)
Out[5]:
tensor([[[ 0,  1],
         [ 6,  7]],

        [[ 2,  3],
         [ 8,  9]],

        [[ 4,  5],
         [10, 11]]])

In [6]: a.permute(2,1,0)
Out[6]:
tensor([[[ 0,  6],
         [ 2,  8],
         [ 4, 10]],

        [[ 1,  7],
         [ 3,  9],
         [ 5, 11]]])

## gather/scatter 示例
https://blog.csdn.net/Teeyohuang/article/details/82186666
https://blog.csdn.net/guofei_fly/article/details/104308528

scatter 其他维度正常走，dim按照index选



## reduce
操作是将高维数据合并成一个数值。如果里面有inf，碰到0就完蛋了。
https://blog.csdn.net/qq_32458499/article/details/79468426
Reduces the tensor data across all machines in such a way that all get the final result.

reduce为False时，对batch内的每个样本单独计算loss，loss的返回值Shape为[N],每一个数对应一个样本的loss。reduce为True时，根据size_average决定对N个样本的loss进行求和还是平均，此时返回的loss是一个数。


