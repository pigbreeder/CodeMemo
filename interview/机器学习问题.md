# 机器学习问题

## 为什么稀疏？regulation
为了能获得比较好的解，人们需要[公式]的先验知识。而稀疏性便是众多先验知识中，最为主要的一种

sparse 代表数据为0，sparse数据的存在让不为0的dense数据聚集在一起；
因为存在数据聚集效应，所以才能学到特征和规律；

如果数据维度很高，噪音很多，原本为0的位置，占比会越来越少，稀疏区域在消失；
对应的dense数据的聚集效应减弱，因为看上去全是数据，看不见产生聚集效应的稀疏隔离区域；

稀疏数据占比减少，导致数据聚集效应的消失，导致特征学习变得困难
稀疏性的高低会影响特征学习效率，自然是很重要的。

三个问题：
- loss是什么
- 学习的是什么
- 训练预测怎么做
## 手写
- lr,softmax
- kmeans(单机中优化性能/SGD快速更新)
- svm(KKT,SMO,软间隔)
- xgboost
- bp
- 交叉熵 KL散度
- EM
- FM/FFM

## 树模型
- 信息熵/基尼系数
- 连续特征处理
- 决策树剪枝
- GBDT和xgboost区别
- 如何处理稀疏特征
- xgboost 如何并行
- GBDT用的都是回归树，如何做分类？ 
一个分类训练一个CART树，然后所有预测结果softmax https://www.zybuluo.com/yxd/note/611571 gbdt详解
- 随机森林(就是行采样，列采样。行采样是选取不同的样本，列采样是选取不同的性质。很少，然后组合而成)，如何选择特征个数
- 树缺失值处理(离散特征/连续特征)
分裂时样本存在缺失（只用不缺失的乘以系数）/分裂完后缺失样本归属（两边都走样本改变权重）/测试（填充），
https://www.zhihu.com/question/34867991 


## LR和SVM
- 高维稀疏特征的时候，lr 的效果会比 gbdt 好，为什么？
lr的正则惩罚性相比于树的正则（对叶子和深度惩罚）抑制某个权重更明显。

- LR为什么都是one-hot? gbdt为什么是离散?
https://blog.csdn.net/u010358304/article/details/80693541
https://www.zhihu.com/question/31989952
lr进行离散化后相当于引入非线性特征，
https://www.cnblogs.com/bentuwuying/p/6616761.html 
https://www.cnblogs.com/zhizhan/p/5038747.html 
LR模型是广义的线性模型，离散化后的特征对异常数据有很强的鲁棒性，对于异常数据和特征稳定都有好 在超高维的特征空间中，很多问题就变为线性可分问题，而从可以极大地提高分类器的能力。对于像神经网络的深度非线性模型则意义不大。因为神经网络中的每个神经元都可以看作是一个近似的离散特征生成器，输出为两种状态：兴奋和抑制。本身就可以进行表示学习，所以就不再需要特征工程了。
单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性
离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性
连续特征离散化，并对离散化的特征进行One-Hot编码

- hinge loss 和 交叉熵的区别联系
- SVM多分类，软间隔
- SVM不同核之间的区别
- SVM适合处理什么样的数据？
高维稀疏，样本少【参数只与支持向量有关，数量少，所以需要的样本少，由于参数跟维度没有关系，所以可以处理高维问题】
- svm手推，核函数是什么，常见有什么
调参： https://blog.csdn.net/szlcw1/article/details/52259668
https://www.jianshu.com/p/f4c7bc6c4ce2
核函数： https://blog.csdn.net/leonis_v/article/details/50688766
具体使用：http://blog.51cto.com/apinetree/1560240

---
## 杂点
- 共轭先验
- LR和NB的区别
- MLE和MAP的区别
- ROC、AUC、P\R在不同数据集上的表现
- 信息增益，信息熵的计算
- 正则化/L0/L1/L2, 高斯和拉普拉斯先验给出了l1和l2正则稀疏性的解释。给出了l2鲁棒性的解释
- k-means 的缺点，大规模怎么做，变分
- 特征选择怎么做
- AUC ROC  recall precision
- 损失函数不可导 如何梯度下降（次梯度）
- 正负样本不平衡
- 克服过拟合/欠拟合
- wide and deep 和 lr+dnn异同
- 集成学习 adaboost/bagging/boosting/stacking
- Adaboost
例子 https://blog.csdn.net/v_JULY_v/article/details/40718799
指数损失是0-1损失函数的一种代理函数
http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/
https://blog.csdn.net/on2way/article/details/48006539
adaboost 一个是样本权重，一个是弱学习器权重，一个是误差率ek计算
adaboost是用来做二分类，多分类使用oneVSone , oneVSall,样本标签是+1 -1


